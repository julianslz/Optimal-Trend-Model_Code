{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/GeostatsGuy/GeostatsPy/blob/master/TCG_color_logo.png?raw=true\" width=\"220\" height=\"240\" />\n",
    "\n",
    "</p>\n",
    "\n",
    "# Subsurface Data Analytics \n",
    "\n",
    "## Spatial Modeling with Spatial Data\n",
    "\n",
    "\n",
    "**Jose Julian Salazar, Equinor Fellow and Graduate Research Assistant at The University of Texas at Austin**\n",
    "\n",
    "**Jesus Ochoa, Technology, Digital & Innovation, Equinor**\n",
    "\n",
    "**Lean Garland, Exploration & Production International, Equinor**\n",
    "\n",
    "**[Michael Pyrcz](https://www.linkedin.com/in/michael-pyrcz-61a648a1), Associate Professor, The University of Texas at Austin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Executive summary\n",
    "\n",
    "The notebook helps to model the geological trend of petrophysical properties for any 2D spatial dataset..\n",
    "\n",
    "Given the vast extension of the area of interest of spatial data and the data paucity it would be beneficial to provide a data-driven approach to model petrophysical and geophsyical properties.\n",
    "\n",
    "The present workflow provides a semi-automatic approach for the user and it uses:\n",
    "\n",
    "* Convolution with moving window to easily model the trend.\n",
    "* Bayesian optimization to automatically optimize the convolution hyperparameters and identify the major direction of continuity.\n",
    "* Genetic algorithms to automatically model the semivariogram.\n",
    "* Simulation and cosimulation using sequential Gaussian simulation.\n",
    "* Visualization functions to interpret and check results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the packages we require to run this Notebook. Next, we will import the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook, please install the packages. Make sure the `requirements.txt` is located in the same folder as this notebook. Then, run the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T15:02:31.921728Z",
     "start_time": "2023-02-07T15:02:31.915744Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Ax-platform as well to run Bayesian optimization. It is Facebook's [Ax](https://ax.dev/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T15:02:31.937686Z",
     "start_time": "2023-02-07T15:02:31.923723Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install ax-platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T15:02:42.604889Z",
     "start_time": "2023-02-07T15:02:31.939680Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geostatspy import geostats\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from OptimalModel import SpatialModeler, OptimizeCosim\n",
    "\n",
    "# Options for pandas\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 30\n",
    "\n",
    "# Visualizations\n",
    "warnings.filterwarnings('ignore')  # Don't show the nan warning\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first simulate porosity (without cosimulation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import\n",
    "\n",
    "Let's import all the required data for the analysis. We will read the T1-T2 interval. Let's define the coordinates, the feature of interest and the cell size. The smaller the cell size, the more time it takes to run the algorithm. Let's settle for 1200 m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T15:02:42.620927Z",
     "start_time": "2023-02-07T15:02:42.604889Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T15:02:42.637767Z",
     "start_time": "2023-02-07T15:02:42.621968Z"
    }
   },
   "outputs": [],
   "source": [
    "xcoor = 'X column'\n",
    "ycoor = 'Y column'\n",
    "main_feat = 'Main'\n",
    "sec_feat = 'Secondary'\n",
    "cell_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up: instantiate the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T16:01:47.015570Z",
     "start_time": "2022-01-19T16:01:47.010582Z"
    }
   },
   "source": [
    "Let's create a folder to store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T15:02:42.653724Z",
     "start_time": "2023-02-07T15:02:42.639761Z"
    }
   },
   "outputs": [],
   "source": [
    "root = \"./results\"\n",
    "os.makedirs(root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T15:02:42.714106Z",
     "start_time": "2023-02-07T15:02:42.671764Z"
    }
   },
   "outputs": [],
   "source": [
    "np.round(df.describe(), 2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before computing the trend model, let's instantiate the object. We will call it `phi_model` and the inputs of the class are the updated dataframe, the column names of the coordinates and feature, the cell size, and the path we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T15:02:42.730064Z",
     "start_time": "2023-02-07T15:02:42.716102Z"
    }
   },
   "outputs": [],
   "source": [
    "sec_model = SpatialModeler(df, xcoor, ycoor, sec_feat, cell_size, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will visualize the data and detect outliers that could negatively impact our model. Using the object we created, we use the function `outlier_detection`. The function is based on the Mahalanobis distance and requires the following inputs:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Parameters\n",
    "----------\n",
    "alpha: float\n",
    "Confidence value between 0. and 1.0. The smaller, the less values are outliers.\n",
    "\n",
    "plot: bool\n",
    "True to plot the classified data.\n",
    "\n",
    "update_data: bool\n",
    "True if you want to remove the outliers points from the dataset.\n",
    "\n",
    "xy_ratio: float\n",
    "X to Y ratio for visualization purposes.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**TIP**: I suggest you set `update_data` to False at the beginning until you find an alpha value that yields good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T15:02:42.933799Z",
     "start_time": "2023-02-07T15:02:42.731061Z"
    }
   },
   "outputs": [],
   "source": [
    "sec_model.spatial_outliers(\n",
    "    alpha=0.001,\n",
    "    plot=True,\n",
    "    xy_ratio=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental variograms before removing outliers based on feature values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T15:02:45.476451Z",
     "start_time": "2023-02-07T15:02:42.935795Z"
    }
   },
   "outputs": [],
   "source": [
    "sec_model.feature_outliers(\n",
    "    selection=1,\n",
    "    update=True,\n",
    "    plot=True,\n",
    "    verbose=True,\n",
    "    contamination=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T18:17:15.335282Z",
     "start_time": "2022-10-13T18:17:12.696806Z"
    }
   },
   "outputs": [],
   "source": [
    "sec_model.declus(1e3, 2e3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T18:27:18.758882Z",
     "start_time": "2022-10-13T18:27:13.131059Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sec_model.vario_plot(extend_half_dist=1.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before computing a trend let's visualize if a trend exists in the first place. We can infer a trend is present if the experimental variogram rises above the sill steeply (i.e., it does not plateau when Y=1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the semivariogram could depict a trend, we have to be aware that if we try to remove a trend when it does not exist, the semivariogram model will be pure nugget effect because nothing will be left to model.\n",
    "\n",
    "You can confirm the trend presence when modeling the variogram you do not obtain large values of nugget effect. Let's assume a trend exists because the variogram model is not pure nugget effect.\n",
    "\n",
    "**This is the first update for fast modeling**\n",
    "We will execute a py file. You will only have to update the py file: you could use PyCharm, VisualStudio, Spyder, or even the NotePad. The  inputs you have to modify are:\n",
    "\n",
    "```python\n",
    "##############################################\n",
    "# INPUT PARAMETERS\n",
    "##############################################\n",
    "self_directory = '.\\OptimalTrendModel\\\\results'  # UPDATE THIS LINE\n",
    "directory = os.path.join(self_directory, \"updated_dataset.pkl\")  # IF YOU REMOVE OUTLIERS, USE THIS LINE\n",
    "\n",
    "xcoor = 'X LOCATION'\n",
    "ycoor = 'Y LOCATION'\n",
    "feature = \"PHIE\"\n",
    "cell_size = 1200\n",
    "min_window = 10\n",
    "max_window = 25\n",
    "underfit = 0.35\n",
    "overfit = 0.42\n",
    "population_size = 8\n",
    "number_of_generations = 10\n",
    "```\n",
    "\n",
    "Definition\n",
    "\n",
    "* **df**: the path to your CSV or EXCEL file\n",
    "* **xcoor, ycoor, cell_size**: the same strings you used above when defining the object.\n",
    "* **min_window, max_window**: the minimum and maximum window sizes for your convolutional moving window\n",
    "* **underfit, overfit**: the range to consider an optimal trend model (it could go from 0.35 to 0.55). If the $$\\frac{\\sigma^2_{trend}}{\\sigma^2_{feature}}$$ is outside the range, there is error.\n",
    "* **population_size**: The number of original solutions to compute. For the first generation and on, the number of solutions computed are half of the population size. Therefore, the larger the population size, the more time it would take to compute\n",
    "* **number_of_generations**: The number of trial solutions you will run.\n",
    "\n",
    "Be sure `trend_modeler.py` is in the same folder as this notebook. You can select the number of CPUs to use (here I am using 8, if you want to use all your computer power, delete the `-n 8`; that is:\n",
    "```python\n",
    "! python -m scoop trend_modeler.py\n",
    "```\n",
    "\n",
    "**WARNING** The following code will consume all your resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T20:46:58.620883Z",
     "start_time": "2022-05-08T20:46:58.606143Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! python -m scoop trend_modeler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T18:32:08.290429Z",
     "start_time": "2022-10-13T18:32:08.273473Z"
    }
   },
   "outputs": [],
   "source": [
    "tm_results = pd.read_pickle(os.path.join(os.getcwd(), \"results\", \"tm_results.pkl\"))\n",
    "tm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T18:32:20.526327Z",
     "start_time": "2022-10-13T18:32:20.504384Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=tm_results['Mean loss'], y=tm_results['Variance loss'],\n",
    "    text=tm_results['Text'], mode=\"markers\",\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Pareto frontier\",\n",
    "    xaxis_title=\"Mean loss\",\n",
    "    yaxis_title=\"Variance loss\", \n",
    ")\n",
    "fig.update_yaxes(autorange=\"reversed\")\n",
    "fig.update_xaxes(autorange=\"reversed\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal window sizes are x=13.9727 and y=22.6484 **cell units**. It took 42 seconds to run! We have to input those window sizes in the next function to obtain the trend map and check the goodness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T18:38:42.758962Z",
     "start_time": "2022-10-13T18:38:37.963472Z"
    }
   },
   "outputs": [],
   "source": [
    "sec_model.model_trend(\n",
    "    xwindow=10,\n",
    "    ywindow=11,\n",
    "    theta=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T18:38:46.075744Z",
     "start_time": "2022-10-13T18:38:45.681271Z"
    }
   },
   "outputs": [],
   "source": [
    "px.imshow(sec_model.trend_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variogram modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the semivariogram model. The model is a requirement for sequential Gaussian simulation. The inputs are:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "bayesian_iterations: int\n",
    "Number of iterations to identify the major direction of continuity between min_azimuth and max_azimuth.\n",
    "\n",
    "min_azimuth, max_azimuth: float\n",
    "Minimum and maximum azimuth values to consider when estimating the major direction of continuity.\n",
    "\n",
    "parallel: bool\n",
    "True to run Bayesian optimization using parallelism. Default is False.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "\n",
    "To avoid getting stuck in local minima, please follow these recommendations:\n",
    "1. Set the min azimuth and max azimuth from 0 to 180 and run the `model_variogram` multiple times. Save the azimuth values you gets. This is the exploration part.\n",
    "2. From the values obtained in 1, update the min and max azimuth values.\n",
    "3. Run `model_variogram` with a reduced range of min and max azimuth values. This is the exploitation part.\n",
    "```\n",
    "\n",
    "The idea is to find the largest 'Major range' possible. The analysis corresponds to the **normal scores of the residual (if a trend exists), or the original feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T18:42:03.761580Z",
     "start_time": "2022-10-13T18:42:03.734653Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sec_model.find_range_lag(\n",
    "    bayesian_iterations=10,\n",
    "    min_azimuth=10,\n",
    "    max_azimuth=50,\n",
    "    load_tensors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are satisfied with the major range of continuty and you set save=True, run the variogram modeling code below. Again, to update your parameters, you will have to edit the py file as you did with `trend_modeler.py`.\n",
    "\n",
    "Definitions\n",
    "```python\n",
    "##############################################\n",
    "# INPUT PARAMETERS\n",
    "##############################################\n",
    "population_size = 20\n",
    "number_of_generations = 10\n",
    "lag_dist_model = 1202.6456\n",
    "```\n",
    "\n",
    "* **population_size**: The number of original solutions to compute. For the first generation and on, the number of solutions computed are half of the population size. Therefore, the larger the population size, the more time it would take to compute\n",
    "* **number_of_generations**: Number of trials to perform to find the optimal variogram model.\n",
    "* **lag_dist_model**: The recommended lag distance given above from the **find_spatial_cont** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T18:42:52.850130Z",
     "start_time": "2022-10-13T18:42:52.844146Z"
    }
   },
   "outputs": [],
   "source": [
    "sec_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T18:45:21.084789Z",
     "start_time": "2022-10-13T18:45:14.693683Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m scoop vario_modeler.py -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, input the parameters in the following dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T18:51:17.865269Z",
     "start_time": "2022-10-13T18:51:17.846321Z"
    }
   },
   "outputs": [],
   "source": [
    "vmodel_denpor = {\n",
    "    \"nug\": 999.05,\n",
    "    \"nst\": 999,\n",
    "    \"it1\": 999,  # 1 for spherical, 2 exponential, 3 gaussian\n",
    "    \"cc1\": 999.45,\n",
    "    \"azi1\": 999.04,\n",
    "    \"hmaj1\": 999.17,\n",
    "    \"hmin1\": 999.1,\n",
    "    \"it2\": 999,\n",
    "    \"cc2\": 999.5,\n",
    "    \"azi2\": 999.04,\n",
    "    \"hmaj2\": 999.98,\n",
    "    \"hmin2\": 999.32,\n",
    "}  # loss 996"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the variogram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T18:51:18.572725Z",
     "start_time": "2022-10-13T18:51:18.550786Z"
    }
   },
   "outputs": [],
   "source": [
    "sec_model.plot_semiv_model(vmodel_denpor, max_distance=10e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** \n",
    "Everytime you run the Bayesian Optimization, you optimum window size (hyperparameters) will change. That is, the solution is non unique because is an optimization problem. However, the final window sizes would be close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Gaussian simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the number of realizations we want from the sequential Gaussian simulation. We set `python=False` to use the robust and faster version from GSLIB. `python=True` uses the academic sequential Gaussian simulation.\n",
    "Also, we set `cosimulation=False`, because we will not constrain the porosity simulations using another feature.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Parameters\n",
    "-----------\n",
    "\n",
    "realizations: int\n",
    "Number of realizations to simulate.\n",
    "\n",
    "vario_model: dict\n",
    "Variogram model in geostatspy format.\n",
    "\n",
    "python: bool\n",
    "True to execute with Python. False to execute with GSLIB (recommended).\n",
    "\n",
    "cosimulation: bool\n",
    "True to perform cosimulation (it requires the realizations of the secondary feature).\n",
    "\n",
    "feat_secondary: str\n",
    "Name of the secondary function provided cosimulation is True.\n",
    "\n",
    "simulation_sec: np.ndarray\n",
    "Realizations from SGS provided cosimulation is True.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Here we will run 10 simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T18:53:31.001779Z",
     "start_time": "2022-10-13T18:51:21.004453Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simulations_denpor, summary_denpor = sec_model.simulation(\n",
    "    realizations=30,\n",
    "    vario_model=vmodel_denpor,\n",
    "    python=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality check\n",
    "### Histogram reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the realizations replicate the histogram of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T18:53:39.925948Z",
     "start_time": "2022-10-13T18:53:36.338251Z"
    }
   },
   "outputs": [],
   "source": [
    "sec_model.histogram_reprod(nbins=10, save_img=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T20:49:14.888973Z",
     "start_time": "2022-05-08T20:49:14.877004Z"
    }
   },
   "outputs": [],
   "source": [
    "sec_model.qq_plot(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial reproducibility and variogram map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we check if our realizations replicate the variogram of the samples. The more number of variograms from the realization we want to plot, the more time it will take.\n",
    "\n",
    "\n",
    "**Parameters**\n",
    "* azimuth_1, azimuth_2: int\n",
    "\n",
    "    Choose from 4 azimuths: 0, 45, 90, 135 degrees.\n",
    "\n",
    "\n",
    "* n_variograms: int\n",
    "\n",
    "    Number of gridded variograms to plot. It should be smaller or equal to the number or realizations.\n",
    "     The larger, the more time to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T21:20:55.303000Z",
     "start_time": "2022-05-08T20:49:14.906552Z"
    }
   },
   "outputs": [],
   "source": [
    "sec_model.vario_reprod(\n",
    "    azimuth_1=90,\n",
    "    azimuth_2=135,\n",
    "    xrange=1.5e5,\n",
    "    vario_model=vmodel_denpor,\n",
    "    n_variograms=30,\n",
    "    save_img=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`simulations` contains all the realizations you perform in a 2D numpy array. On the other hand, `summary` is a tensor that contains the following information:\n",
    "* P10 map\n",
    "* P50 map\n",
    "* P90 map\n",
    "* Uncertainty P90-P10\n",
    "* Mean of all realizations\n",
    "* Trend\n",
    "\n",
    "Furthermore, you don't have to worry about adding the trend back because it is done automatically by the class. Let's perform a quality check of our realizations.\n",
    "\n",
    "We can get an xarray file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the summary plots in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:45:09.939475Z",
     "start_time": "2022-09-06T20:45:09.932761Z"
    }
   },
   "outputs": [],
   "source": [
    "width = 1500\n",
    "height = 600\n",
    "from plotly.subplots import make_subplots\n",
    "# from IPython.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:45:35.137281Z",
     "start_time": "2022-09-06T20:45:28.428435Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=3, shared_yaxes=True, horizontal_spacing=0.01,\n",
    "    x_title=sec_model._Visualization._xcoor,\n",
    "    y_title=sec_model._Visualization._ycoor,\n",
    "    subplot_titles=(\"P10\", \"P50\", \"P90\")\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=np.flipud(summary_denpor[0]),\n",
    "        x=sec_model._Visualization._coorsx,\n",
    "        y=sec_model._Visualization._coorsy,\n",
    "        coloraxis = \"coloraxis\",\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sec_model._Visualization._df_hover[sec_model._Visualization._xcoor],\n",
    "        y=sec_model._Visualization._df_hover[sec_model._Visualization._ycoor],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='black',\n",
    "            showscale=False,\n",
    "            size=5,\n",
    "            symbol='circle',\n",
    "            opacity=0.8\n",
    "        ),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "#########\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=np.flipud(summary_denpor[1]),\n",
    "        x=sec_model._Visualization._coorsx,\n",
    "        y=sec_model._Visualization._coorsy,\n",
    "        coloraxis = \"coloraxis\",\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sec_model._Visualization._df_hover[sec_model._Visualization._xcoor],\n",
    "        y=sec_model._Visualization._df_hover[sec_model._Visualization._ycoor],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='black',\n",
    "            showscale=False,\n",
    "            size=5,\n",
    "            symbol='circle',\n",
    "            opacity=0.8\n",
    "        ),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "#########\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=np.flipud(summary_denpor[2]),\n",
    "        x=sec_model._Visualization._coorsx,\n",
    "        y=sec_model._Visualization._coorsy,\n",
    "        coloraxis = \"coloraxis\",\n",
    "        zmin=np.nanmin(summary_denpor),\n",
    "        zmax=np.nanmax(summary_denpor),\n",
    "    ),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sec_model._Visualization._df_hover[sec_model._Visualization._xcoor],\n",
    "        y=sec_model._Visualization._df_hover[sec_model._Visualization._ycoor],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='black',\n",
    "            showscale=False,\n",
    "            size=5,\n",
    "            symbol='circle',\n",
    "            opacity=0.8\n",
    "        ),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "# Add dropdown\n",
    "fig.update_layout(\n",
    "    title=sec_model._Visualization.primary_feat,\n",
    "    autosize=False,\n",
    "    width=width,\n",
    "    height=height,\n",
    "    coloraxis = {'colorscale':'plasma'},\n",
    "    font=dict(size=13)\n",
    "\n",
    ")\n",
    "fig.update_annotations(font_size=20)\n",
    "fig.update_xaxes(ticks='inside')\n",
    "fig.update_yaxes(ticks='inside')\n",
    "fig.show()\n",
    "fig.write_image(\"fig1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T18:53:57.565099Z",
     "start_time": "2022-10-13T18:53:55.537793Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sec_model.summary_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local probability of exceedance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the probability of exceedance where we specify a threshold porosity value and calculate the probability of exceeding that value at all locations. We will typically select critical thresholds, such as a net-to-gross threshold.\n",
    "\n",
    "Source: [Michael Pyrcz](https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/GeostatsPy_simulation_postsim.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:53:07.686186Z",
     "start_time": "2022-09-06T20:53:04.638590Z"
    }
   },
   "outputs": [],
   "source": [
    "sec_model.local_prob_exceedance(threshold=7.5, save_img=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the summary results with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T21:21:04.534215Z",
     "start_time": "2022-05-08T21:21:04.518338Z"
    }
   },
   "outputs": [],
   "source": [
    "sec_array = sec_model.save_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primary feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that porosity helps predicting VSH for demonstration purposes. The new dataset will contain the new feature to model, and a secondary feature (porosity in this case). To perform cosimulation, you should have performed the sequential Gaussian simulation of the secondary feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the dataset, but now it will hold two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:53:16.372473Z",
     "start_time": "2022-09-06T20:53:16.350533Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = sec_model.dataset.copy()\n",
    "\n",
    "df2 = df2[['UWI', xcoor, ycoor, main_feat, sec_feat]]\n",
    "df2.dropna(inplace=True)\n",
    "df2.drop_duplicates(inplace=True, subset=[xcoor, ycoor], ignore_index=True)\n",
    "\n",
    "primary_model = SpatialModeler(df2, xcoor, ycoor, main_feat, cell_size, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following operations are the same as before and are required to perform cosimulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:53:18.995140Z",
     "start_time": "2022-09-06T20:53:18.976190Z"
    }
   },
   "outputs": [],
   "source": [
    "primary_model.spatial_outliers(\n",
    "    alpha=0.001,\n",
    "    plot=True,\n",
    "    update=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:53:19.637895Z",
     "start_time": "2022-09-06T20:53:19.178471Z"
    }
   },
   "outputs": [],
   "source": [
    "primary_model.feature_outliers(\n",
    "    selection=2,\n",
    "    update=False,\n",
    "    plot=False,\n",
    "    verbose=True,\n",
    "    contamination=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:53:19.765553Z",
     "start_time": "2022-09-06T20:53:19.752588Z"
    }
   },
   "outputs": [],
   "source": [
    "primary_model.vario_plot(extend_half_dist=1.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:53:23.233331Z",
     "start_time": "2022-09-06T20:53:20.156968Z"
    }
   },
   "outputs": [],
   "source": [
    "primary_model.declus(8e4, 9e4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T21:21:08.023327Z",
     "start_time": "2022-05-08T21:21:08.007875Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! python -m scoop trend_modeler.py -n 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T21:21:08.049942Z",
     "start_time": "2022-05-08T21:21:08.025321Z"
    }
   },
   "outputs": [],
   "source": [
    "# tm_results_primary = pd.read_pickle(os.path.join(os.getcwd(), \"results\", \"tm_results.pkl\"))\n",
    "# fig = go.Figure()\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=tm_results_primary['Mean loss'], y=tm_results_primary['Variance loss'],\n",
    "#     text=tm_results_primary['Text'], mode=\"markers\",\n",
    "# ))\n",
    "\n",
    "# fig.update_layout(\n",
    "#     title=\"Pareto frontier\",\n",
    "#     xaxis_title=\"Mean loss\",\n",
    "#     yaxis_title=\"Variance loss\",\n",
    "# )\n",
    "# fig.update_yaxes(autorange=\"reversed\")\n",
    "# fig.update_xaxes(autorange=\"reversed\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:53:29.460255Z",
     "start_time": "2022-09-06T20:53:25.102015Z"
    }
   },
   "outputs": [],
   "source": [
    "primary_model.model_trend(\n",
    "    xwindow=99,\n",
    "    ywindow=99,\n",
    "    theta=99\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variogram modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Major range = 16097.30. Azimuth = 116.63\n",
    "Major range = 16063.39. Azimuth = 116.28\n",
    "\n",
    "Recommended lag distance = 1209.00.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:55:18.304163Z",
     "start_time": "2022-09-06T20:55:18.269504Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "primary_model.find_range_lag(\n",
    "    bayesian_iterations=10,\n",
    "    min_azimuth=110,\n",
    "    max_azimuth=125,\n",
    "    load_tensors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T21:21:12.129484Z",
     "start_time": "2022-05-08T21:21:12.103854Z"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m scoop vario_modeler.py -n 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:55:20.110200Z",
     "start_time": "2022-09-06T20:55:20.094575Z"
    }
   },
   "outputs": [],
   "source": [
    "vmodel_primary = {\n",
    "    \"nug\": 0.08,\n",
    "    \"nst\": 2,\n",
    "    \"it1\": 3,\n",
    "    \"cc1\": 0.78,\n",
    "    \"azi1\": 116.28,\n",
    "    \"hmaj1\": 14389,\n",
    "    \"hmin1\": 4482.31,\n",
    "    \"it2\": 3,\n",
    "    \"cc2\": 0.14,\n",
    "    \"azi2\": 116.28,\n",
    "    \"hmaj2\": 16063.39,\n",
    "    \"hmin2\": 11606.91,\n",
    "}  #  1188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:55:21.949402Z",
     "start_time": "2022-09-06T20:55:21.802311Z"
    }
   },
   "outputs": [],
   "source": [
    "primary_model.plot_semiv_model(vmodel_primary, max_distance=150e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance reduction factor for cosimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T21:21:12.285792Z",
     "start_time": "2022-05-08T21:21:12.270444Z"
    }
   },
   "outputs": [],
   "source": [
    "# optim_cosim = OptimizeCosim(\n",
    "#     primary_model, sec_feat, simulations_denpor, vmodel_primary\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T21:21:12.301748Z",
     "start_time": "2022-05-08T21:21:12.287783Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# results = optim_cosim.varred_optim(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T21:21:12.317706Z",
     "start_time": "2022-05-08T21:21:12.304739Z"
    }
   },
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "RMSE\n",
    "Error varred=1.0: 1.50\n",
    "Error varred=0.6: 1.26\n",
    "Variance input: 0.95\n",
    "\n",
    "Target var is the naive var (not declustered)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosimulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will perform cosimulation. To do so, you can:\n",
    "* either use the recommended GSLIB version (`python=False`)\n",
    "* set `cosimulation=True`\n",
    "* feat_secondary is the name of the secondary feature (PHIE in this case)\n",
    "* simulation_sec = the realizations of the secondary feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:49:03.957352Z",
     "start_time": "2022-09-06T20:55:29.584718Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simulations_primary, summary_primary = primary_model.simulation(\n",
    "    realizations=30,\n",
    "    vario_model=vmodel_primary,\n",
    "    python=True,\n",
    "    cosimulation=True,\n",
    "    feat_secondary=sec_feat,\n",
    "    simulation_sec=simulations_denpor,\n",
    "    varred=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:49:18.159631Z",
     "start_time": "2022-09-06T21:49:14.732776Z"
    }
   },
   "outputs": [],
   "source": [
    "primary_model.histogram_reprod(nbins=10, save_img=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T22:05:32.706189Z",
     "start_time": "2022-05-08T22:05:32.691396Z"
    }
   },
   "outputs": [],
   "source": [
    "# primary_model.qq_plot(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial continuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T22:05:32.721802Z",
     "start_time": "2022-05-08T22:05:32.709180Z"
    }
   },
   "outputs": [],
   "source": [
    "# primary_model.vario_reprod(\n",
    "#     azimuth_1=0,\n",
    "#     azimuth_2=45,\n",
    "#     xrange=1.5e5,\n",
    "#     vario_model=vmodel_denpor,\n",
    "#     n_variograms=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T22:05:32.737762Z",
     "start_time": "2022-05-08T22:05:32.722804Z"
    }
   },
   "outputs": [],
   "source": [
    "# primary_model.vario_reprod(\n",
    "#     azimuth_1=90,\n",
    "#     azimuth_2=135,\n",
    "#     xrange=1.5e5,\n",
    "#     vario_model=vmodel_denpor,\n",
    "#     n_variograms=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T22:05:32.753533Z",
     "start_time": "2022-05-08T22:05:32.739783Z"
    }
   },
   "outputs": [],
   "source": [
    "# primary_model.vario_map(200, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:50:38.944173Z",
     "start_time": "2022-09-06T21:50:33.922950Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=3, shared_yaxes=True, horizontal_spacing=0.01,\n",
    "    x_title=primary_model._Visualization._xcoor,\n",
    "    y_title=primary_model._Visualization._ycoor,\n",
    "    subplot_titles=(\"P10\", \"P50\", \"P90\")\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=np.flipud(summary_primary[0]),\n",
    "        x=primary_model._Visualization._coorsx,\n",
    "        y=primary_model._Visualization._coorsy,\n",
    "        coloraxis = \"coloraxis\",\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=primary_model._Visualization._df_hover[primary_model._Visualization._xcoor],\n",
    "        y=primary_model._Visualization._df_hover[primary_model._Visualization._ycoor],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='black',\n",
    "            showscale=False,\n",
    "            size=5,\n",
    "            symbol='circle',\n",
    "            opacity=0.8\n",
    "        ),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "#########\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=np.flipud(summary_primary[1]),\n",
    "        x=primary_model._Visualization._coorsx,\n",
    "        y=primary_model._Visualization._coorsy,\n",
    "        coloraxis = \"coloraxis\",\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=primary_model._Visualization._df_hover[primary_model._Visualization._xcoor],\n",
    "        y=primary_model._Visualization._df_hover[primary_model._Visualization._ycoor],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='black',\n",
    "            showscale=False,\n",
    "            size=5,\n",
    "            symbol='circle',\n",
    "            opacity=0.8\n",
    "        ),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "#########\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=np.flipud(summary_primary[2]),\n",
    "        x=primary_model._Visualization._coorsx,\n",
    "        y=primary_model._Visualization._coorsy,\n",
    "        coloraxis = \"coloraxis\",\n",
    "        zmin=np.nanmin(summary_primary),\n",
    "        zmax=np.nanmax(summary_primary),\n",
    "    ),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=primary_model._Visualization._df_hover[primary_model._Visualization._xcoor],\n",
    "        y=primary_model._Visualization._df_hover[primary_model._Visualization._ycoor],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='black',\n",
    "            showscale=False,\n",
    "            size=5,\n",
    "            symbol='circle',\n",
    "            opacity=0.8\n",
    "        ),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "# Add dropdown\n",
    "fig.update_layout(\n",
    "    title=primary_model._Visualization.primary_feat,\n",
    "    autosize=False,\n",
    "    width=width,\n",
    "    height=height,\n",
    "    coloraxis = {'colorscale':'plasma'},\n",
    "    font=dict(size=13)\n",
    "\n",
    ")\n",
    "fig.update_annotations(font_size=20)\n",
    "fig.update_xaxes(ticks='inside')\n",
    "fig.update_yaxes(ticks='inside')\n",
    "fig.show()\n",
    "fig.write_image(\"fig2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T22:05:37.471511Z",
     "start_time": "2022-05-08T22:05:37.455582Z"
    }
   },
   "outputs": [],
   "source": [
    "# primary_model.summary_plots()  # Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local probability of exceendance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T22:05:37.499010Z",
     "start_time": "2022-05-08T22:05:37.474169Z"
    }
   },
   "outputs": [],
   "source": [
    "# primary_model.local_prob_exceedance(threshold=4.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cosimulation plots\n",
    "\n",
    "Let's visualize if the linear correlation holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:50:49.052882Z",
     "start_time": "2022-09-06T21:50:40.891201Z"
    }
   },
   "outputs": [],
   "source": [
    "primary_model.cosim_comparison(simulations_denpor, sec_feat, 400, save_img=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results\n",
    "Let's save the summary dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T22:05:40.719416Z",
     "start_time": "2022-05-08T22:05:40.704397Z"
    }
   },
   "outputs": [],
   "source": [
    "# primary_model.save_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgment\n",
    "\n",
    "Jose Julian Salazar would like to thank Equinor for funding his Ph.D. studies and providing the data required for this workflow.\n",
    "\n",
    "Questions?\n",
    "\n",
    "Please contact me at jsalazarn@austin.utexas.edu. I am happy to discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
